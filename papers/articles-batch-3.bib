@article{rayyan-384037964,
  title={An architecture for self-managing microservices - Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
  year={2015},
  pages={19–24},
  author={Toffetti, Giovanni and Brunner, Sandro and Bl\"{o}chlinger, Martin and Dudouet, Florian and Edmonds, Andrew},
  url={https://doi-org.proxy-ub.rug.nl/10.1145/2747470.2747474},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={AIMC '15},
  abstract={Running applications in the cloud efficiently requires much more than deploying software in virtual machines. Cloud applications have to be continuously managed: 1) to adjust their resources to the incoming load and 2) to face transient failures replicating and restarting components to provide resiliency on unreliable infrastructure. Continuous management monitors application and infrastructural metrics to provide automated and responsive reactions to failures (health management) and changing environmental conditions (auto-scaling) minimizing human intervention.In the current practice, management functionalities are provided as infrastructural or third party services. In both cases they are external to the application deployment. We claim that this approach has intrinsic limits, namely that separating management functionalities from the application prevents them from naturally scaling with the application and requires additional management code and human intervention. Moreover, using infrastructure provider services for management functionalities results in vendor lock-in effectively preventing cloud applications to adapt and run on the most effective cloud for the job.In this position paper we propose a novel architecture that enables scalable and resilient self-management of microservices applications on cloud.},
  doi={10.1145/2747470.2747474},
  booktitle={Proceedings of the 1st International Workshop on Automated Incident Management in Cloud},
  chapter={0}
}

@article{rayyan-384037965,
  title={Data-Driven Repricing Strategies in Competitive Markets: An Interactive Simulation Platform - Proceedings of the Eleventh ACM Conference on Recommender Systems},
  year={2017},
  pages={355–357},
  author={Boissier, Martin and Schlosser, Rainer and Podlesny, Nikolai and Serth, Sebastian and Bornstein, Marvin and Latt, Johanna and Lindemann, Jan and Selke, Jan and Uflacker, Matthias},
  url={https://doi-org.proxy-ub.rug.nl/10.1145/3109859.3109979},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={RecSys '17},
  keywords={simulation, oligopoly competition, microservice architecture, dynamic pricing, demand learning},
  abstract={Modern e-commerce platforms pose both opportunities as well as hurdles for merchants. While merchants can observe markets at any point in time and automatically reprice their products, they also have to compete simultaneously with dozens of competitors. Currently, retailers lack the possibility to test, develop, and evaluate their algorithms appropriately before releasing them into the real world. At the same time, it is challenging for researchers to investigate how pricing strategies interact with each other under heavy competition. To study dynamic pricing competition on online marketplaces, we built an open simulation platform. To be both flexible and scalable, the platform has a microservice-based architecture and handles large numbers of competing merchants and arriving consumers. It allows merchants to deploy the full width of pricing strategies, from simple rule-based strategies to more sophisticated data-driven strategies using machine learning. Our platform enables analyses of how a strategy's performance is affected by customer behavior, price adjustment frequencies, the competitors' strategies, and the exit/entry of competitors. Moreover, our platform allows to study the long-term behavior of self-adapting strategies.},
  doi={10.1145/3109859.3109979},
  booktitle={Proceedings of the Eleventh ACM Conference on Recommender Systems},
  chapter={0}
}

@article{rayyan-384037966,
  title={Investigating Performance Overhead of Distributed Tracing in Microservices and Serverless Systems - Companion of the 16th ACM/SPEC International Conference on Performance Engineering},
  year={2025},
  pages={162–166},
  author={N\~{o}u, Anders and Talluri, Sacheendra and Iosup, Alexandru and Bonetta, Daniele},
  url={https://doi-org.proxy-ub.rug.nl/10.1145/3680256.3721316},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ICPE '25},
  keywords={distributed tracing, instrumentation, open telemetry, performance},
  abstract={Distributed tracing is crucial to achieve observability in modern distributed systems. However, its adoption introduces performance trade-offs, impacting throughput and latency. This paper investigates the overhead of distributed tracing in microservices and serverless applications. We provide an analysis of the popular OpenTelemetry and Elastic APM distributed tracing frameworks, evaluating their performance impact on microservices and serverless workloads. We highlight and categorize the primary sources of overhead and measure their contribution to performance degradation. The results reveal significant throughput reductions (19-80\%) and latency increases (up to 175\%) depending on application configurations and execution environments. Our findings reveal that serializing trace data for export is the largest cause of overhead.},
  doi={10.1145/3680256.3721316},
  booktitle={Companion of the 16th ACM/SPEC International Conference on Performance Engineering},
  chapter={0}
}

@article{rayyan-384037967,
  title={Using Cloud Native Technologies to Understand the Performance of Cloud Native Technologies - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
  year={2023},
  pages={261},
  author={Klein, Cristian},
  url={https://doi-org.proxy-ub.rug.nl/10.1145/3578245.3584921},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ICPE '23 Companion},
  keywords={cloud native, experimentation, microservices, traffic management},
  abstract={The ambition of this talk is to seed discussions around how cloud native technologies can help research on performance engineering, but also what are the interesting performance engineering challenges to solve with cloud native technologies.Cloud native technologies are building blocks for creating a modern environment for hosting containerized applications. Amongst others, great focus is placed on observability, which allows engineers to collect and analyze massive amounts of performance data in near real-time. Take as an example service meshes, which are a layer 7 network platform for containerized applications. Service meshes not only allow traffic engineering, but also add observability on top of a microservice application. Amongst other, this allows understanding traffic patterns between microservices, including upstream-downstream relationships, request rate, etc. without writing a single line of code.This talk discusses how cloud native technologies may help researchers in performance engineering. The benefits are three-fold. They allow researchers - e.g., PhD students - to be more productive, by getting the mechanism of collecting performance data out of the way. They improve collaboration because the effects of changing a parameter can be visualized in near real time. Finally, experiments are based on proven technologies with skills more widely available, which helps reproducibility.These benefits are illustrated through our research on adaptive service meshes. Indeed, service meshes have many parameters which impact performance. Discussions with practitioners revealed a gap in understanding on how to effectively choose these parameters. We therefore proposed an adaptive controller that configures a service mesh so as to maintain a target tail response time.},
  doi={10.1145/3578245.3584921},
  booktitle={Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
  chapter={0}
}

@article{rayyan-384037968,
  title={ESTELLE: An Efficient and Cost-effective Cloud Log Engine - Companion of the 2024 International Conference on Management of Data},
  year={2024},
  pages={201–213},
  author={Zhang, Yupu and Cong, Guanglin and Qu, Jihan and Xu, Ran and Fu, Yuan and Li, Weiqi and Hu, Feiran and Liu, Jing and Zhang, Wenliang and Zheng, Kai},
  url={https://doi-org.proxy-ub.rug.nl/10.1145/3626246.3653387},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={SIGMOD '24},
  keywords={bloom filter, cloud-native, cost-effectiveness, index framework, log engine},
  abstract={With the advancement of cloud computing, more and more enterprises are adopting cloud services to build a variety of applications. Monitoring and observability are integral to the complex and fragile cloud-native architecture. As an extremely important data source for both, logs play an indispensable role in applications such as code debugging, root cause analysis, troubleshooting, and trend analysis. However, the inherent characteristic of cloud logs, with TB-level daily data production per user and continuous growth over time and with business, poses core challenges for log engines. Traditional log management systems are inadequate for handling the requirements of massive log data high-frequency writing and storage, along with low-frequency retrieval and analysis in cloud environments. Exploring a low-cost, high-performance cloud-native log engine solution is an extremely extraordinary challenging task. To tackle these challenges, we propose a cost-effective cloud-native log engine, called ESTELLE, equipped with a low-cost pluggable log index framework. This engine features a compute-storage separation and read-write separation architecture, enabling linear scalability. We designed a near-lock-free writing process for handling high-frequency writing demands of massive logs. Object storage is used to significantly reduce storage costs. We also tailored ESTELLE Log Bloom filter and approximate inverted index for this cloud-native engine, applying them flexibly to enhance query efficiency and optimize various queries. Extensive experiments on real open-source log datasets have demonstrated that the ESTELLE Log Engine achieves ultra-high single-core CPU write speeds and pretty low storage costs. Furthermore, when equipped with the complete index framework, it also maintains fairly low query latency across various log scenarios.},
  doi={10.1145/3626246.3653387},
  booktitle={Companion of the 2024 International Conference on Management of Data},
  chapter={0}
}

@article{rayyan-384037969,
  title={Towards First-Class Architectural Connectors: The Case for Self-Adaptive Service Meshes - Proceedings of the XXXV Brazilian Symposium on Software Engineering},
  year={2021},
  pages={404–409},
  author={Mendonca, Nabor and Aderaldo, Carlos},
  url={https://doi-org.proxy-ub.rug.nl/10.1145/3474624.3477072},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={SBES '21},
  keywords={architectural connectors, self-adaptive systems, service mesh},
  abstract={The development of architectural connectors as first-class software entities is a long-standing software engineering promise that has not been fully realized thus far. This observation is especially true in self-adaptive software systems, where most advances within academia and industry have targeted software components as the primary locus for run-time adaptation. In this paper, we revisit the evolutionary history of architectural connectors and discuss the challenges of implementing architectural connectors as first-class self-adaptation entities in the domain of modern microservice applications. We then make a case for our ongoing work on a cloud-native self-adaptive service mesh framework that builds on recent container orchestration, self-adaptation, and service mesh technologies.},
  doi={10.1145/3474624.3477072},
  booktitle={Proceedings of the XXXV Brazilian Symposium on Software Engineering},
  chapter={0}
}

@article{rayyan-384037970,
  title={Federated Single Sign-On and Zero Trust Co-design for AI and HPC Digital Research Infrastructures - Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
  year={2025},
  pages={1756–1764},
  author={Alam, Sadaf R. and Woods, Christopher and Williams, Matt and Moore, Dave and Prior, Isaac and Williams, Ethan and Price, Anna and Womack, James and McIntosh-Smith, Simon and Yang-Turner, Fan and Pryor, Matt and Livenson, Ilja},
  url={https://doi-org.proxy-ub.rug.nl/10.1109/SCW63240.2024.00220},
  publisher={IEEE Press},
  series={SC-W '24},
  keywords={AI, Cybersecurity, Federation, HPC, IAM, Workflows, Zero trust architecture (ZTA)},
  abstract={Scientific workflows have become highly heterogenous, leveraging distributed facilities such as High Performance Computing (HPC), Artificial Intelligence (AI), Machine Learning (ML), scientific instruments (data-driven pipelines) and edge computing. As a result, Identity and Access Management (IAM) and Cybersecurity challenges across the diverse hardware and software stacks are growing. Nevertheless, scientific productivity relies on lowering access barriers via seamless, single sign-on (SSO) and federated login while ensuring access controls and compliance. We present an implementation of a federated IAM solution, which is coupled with multiple layers of security controls, multi-factor authentication, cloud-native protocols, and time-limited role-based access controls (RBAC) that has been co-designed and deployed for the Isambard-AI and HPC supercomputing Digital Research Infrastructures (DRIs) in the UK. Isambard DRIs as a national research resource are expected to comply with regulatory frameworks. Implementation details for monitoring, alerting and controls are outlined in the paper alongside selected user stories for demonstrating IAM workflows for different roles.},
  doi={10.1109/SCW63240.2024.00220},
  booktitle={Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
  chapter={0}
}

@article{rayyan-384037971,
  title={Understanding the Energy Consumption of Cloud-native Software Systems - Proceedings of the 16th ACM/SPEC International Conference on Performance Engineering},
  year={2025},
  pages={309–319},
  author={Andringa, Lars and Setz, Brian and Andrikopoulos, Vasilios},
  url={https://doi-org.proxy-ub.rug.nl/10.1145/3676151.3719371},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  series={ICPE '25},
  keywords={cloud-native applications, energy consumption, kubernetes, open stack, virtualization},
  abstract={As the dependence on software systems running on cloud data centers grows on a daily basis, there is an increasingly stronger motivation to reduce their energy consumption. A necessary but not trivial step in this direction is understanding how energy is consumed in virtualized, multi-tenant environments such as the one provisioned in the cloud. Prior work focuses on isolated, non-virtualized systems and is difficult to transfer to this context. A number of industry-led approaches have appeared in the meantime in terms of tools and technological stacks building on the concept of observability as the means to achieve this goal. This paper discusses our approach in adopting one such stack and consequently assessing it for fitness to purpose through an experimental procedure. To this effect, we deploy a cloud-native application on a private cloud infrastructure instrumented for measuring energy consumption through a combination of hardware and software means. We combine the information from these instrumentation points into a mapping model to deal with the different virtualization layers and compare the model against the values reported by the observability stack. Furthermore, we use our model to attribute energy consumption across the virtualization layers and understand how energy is consumed at each one.},
  doi={10.1145/3676151.3719371},
  booktitle={Proceedings of the 16th ACM/SPEC International Conference on Performance Engineering},
  chapter={0}
}

